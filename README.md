<img src="./imgs/Mypic2.jpg" align="right" width="16%"/>
<!-- <br />
<br /> -->

<!-- <p style="text-align:left;"> -->
<!-- <p align="justify"> -->
 I am a research associate (postdoc) at the [University of Edinburgh](https://www.ed.ac.uk/).  My research focuses on efficient machine learning for robotics including deep reinforcement learning and computer vision. These days, I am particularly interested in neural ODE and its applications in developing scalable imitation learning algorithms, physics-informed neural networks (PINN) to learn residual dynamics and residual actions, also deep learning approaches for visual object perception, segmentation and grasp pose prediction in highly cluttered environments. I have evaluated my works on different robotic platforms, including Nextage, YuMi, robotic arms (UR5, Franka), legged robots (biped and quadruped) and soft robots. 
 <!-- </p> -->
<br />
<br />

 [Google scholar](https://scholar.google.com/citations?user=2aY06V4AAAAJ&hl=en) / [Linkedin](https://www.linkedin.com/in/mohammadreza-kasaei-0a891ab6/) / [AIR-Lab](https://advanced-intelligent-robotics-lab.gitlab.io/)/ [Github](https://github.com/MohammadKasaei)/ [Youtube](http://www.youtube.com/@mohammadrezakasaei2275)
<br />
<br />

# Recent Works

<table style="border:hidden;cellspacing=0; cellpadding=0;">
  <tr>
    <!-- <td width = "33%"><img src="./imgs/Simul.gif"/></td> -->
    <td width = "33%"><img src="./imgs/YuMi.gif"/></td>
    <td width = "33%"><img src="./imgs/cluttered.gif"/></td>
    <td width = "33%"><img src="./imgs/Nextage.gif"/></td>
  </tr>
  <tr>
    <td width = "33%"><img src="./imgs/humanoid.gif"/></td>
    <td width = "33%"><img src="./imgs/Talos.gif"/></td>
    <td width = "33%"><img src="./imgs/A1.gif"/></td>
  </tr>
  <tr>
    <!-- <td width = "33%"><img src="./imgs/softRobot.gif"/></td> -->
    <td width = "33%"><img src="./imgs/tactip_grasp.gif"/></td>
    <td width = "33%"><img src="./imgs/softRobot22.gif"/></td>
    <td width = "33%"><img src="./imgs/throwing.gif"/></td>    
  </tr>
  
</table>

# Latest News

- **August 2024:** My paper, <span style="color:#006600"> <b>  SoftManiSim: A Fast Simulation Framework for Multi-Segment Continuum Manipulators Tailored for Robot Learning,</b></span>  has been accepted for presentation at the 2024 Conference on Robot Learning [(**CoRL 2024**)](https://www.corl.org/). A big thank you to my collaborators!
  
- **June 2024:** My research team has had <span style="color:#006600"> <b> two papers accepted </b></span> for presentation at the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems  [(**IROS 2024**)](https://iros2024-abudhabi.org/). A big thank you to my students and collaborators!
 
- **June 2024:** Our paper titled <span style="color:#006600"> <b> Neural ODE-based Imitation Learning (NODE-IL): Data-Efficient
Imitation Learning for Long-Horizon Multi-Skill Robot Manipulation</b></span> has been accepted for publication in the [**IROS2024**](https://iros2024-abudhabi.org)! 


- **June 2024:** Our paper titled <span style="color:#006600"> <b> Learning Fine Pinch-Grasp Skills Using Tactile Sensing from A Few
Real-world Demonstrations</b></span> has been accepted for publication in the [**IROS2024**](https://iros2024-abudhabi.org)! 
  
- **May 2024:** Our poster titled <span style="color:#006600"> <b> A Novel Dual-Arm Adaptable Impedance Control for Human-Robot Tele-Cooperation</b></span>  got the best porster award at the [(**ICRA 2024**)](https://www.icra2024.org/) workshop on human-centric multilateral teleoperation augmentation! 


- **April 2024:** Our paper titled <span style="color:#006600"> <b> Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains</b></span>  has been accepted for publication in the [**Journal of Intelligent & Robotic Systems**](https://link.springer.com/content/pdf/10.1007/s10846-024-02092-5.pdf)! 

- **January 2024:** My research team has had <span style="color:#006600"> <b> three papers accepted </b></span> for presentation at the 2024 International Conference on Robotics and Automation [(**ICRA 2024**)](https://www.icra2024.org/). A big thank you to my students and collaborators!
  
- **October 2023:** I have successfully integrated my <span style="color:#006600"> <b> In-box Grasp Pose Prediction Pipline </b></span> on ABB Yumi Mobile manipulator for the [Harmony Project](https://harmony-eu.org/). The video is available online [Here](https://youtu.be/JD7b1nMzVdY) and the code is available online [Here](https://github.com/MohammadKasaei/Harmony_InBoxGrasping_ROS2.git).


- **September 2023:** We proposed <span style="color:#006600"> <b> TiV-ODE: A Neural ODE-based Approach for Controllable Video Generation From Text-Image Pairs </b></span>. The paper has been accepted for presentation in [**ICRA 2024**](https://www.icra2024.org/) and is available online [Here](https://arxiv.org/pdf/2303.05323.pdf).
  
- **September 2023:** We proposed a framework for <span style="color:#006600"> <b> Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios </b></span>. The paper has been accepted for presentation in [**ICRA 2024**](https://www.icra2024.org/).

- **September 2023:** We proposed <span style="color:#006600"> <b> Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance Control </b></span>. The paper has been accepted for presentation in [**ICRA 2024**](https://www.icra2024.org/).

 - **September 2023:** We proposed <span style="color:#006600"> <b> NODE-IL: A Data-Efficient Imitation Learning Framework for Long-Horizon Multi-Skills Robot Manipulation </b></span>. The paper is under review.


 - **August 2023:** Our paper entitled <span style="color:#006600"> <b> A Data-efficient Neural ODE Framework for Optimal Control of Soft Manipulators </b></span> got accepted for presentation at [**(CoRL 2023)**](https://www.corl2023.org/). The paper is available online [Here](https://openreview.net/pdf?id=PalhNjBJqv).
 
 - **August 2023:**  Our paper entitled <span style="color:#006600"> <b>Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter </b></span> got accepted for presentation at [**(CoRL 2023)**](https://www.corl2023.org/). The paper is available online [Here](https://openreview.net/pdf?id=j2AQ-WJ_ze).
 
<!--    
  - **July 2023:** We proposed <span style="color:#006600"> <b> Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data</b></span>. The paper is available online [Here](https://arxiv.org/abs/2307.04619).
  
  - **June 2023:** Our poster, <span style="color:#006600"> <b> A Novel Extensible Flexible Arm for Robotic </b></span> got the best poster award in [Soft Growing Robots:](https://www.growing-robots.com/) From Search-and-Rescue to Intraluminal Interventions in [(**ICRA 2023**)](https://www.icra2023.org/).  

  - **March 2023:** Our paper entitled  <span style="color:#006600"> <b> Learning Hybrid Locomotion Skills -Learn to Exploit Residual Actions and Modulate Model-based Gait Control</b></span> got accepted to Frontiers Robotics and AI!

  - **February 2023:** Mohammadreza Kasaei gave a talk at IPAB workshop in the University of Edinburgh, UK entitled **Data-efficient Non-parametric Modelling and Control of an Extensible Soft Manipulator**.
  
  - **January 2023:** My research team has had four papers accepted for presentation at the 2023 International Conference on Robotics and Automation [(**ICRA 2023**)](https://www.icra2023.org/). 
  
  - **November 2022:**: Mohammadreza Kasaei gave an invited talk at the University of Aveiro, Portugal in a Seminar in Robotics and Intelligent Systems on Robotics for Society: Recent Research in Locomotion and Manipulation.
  
  - **October 2022:** Our paper entitled  <span style="color:#006600"> <b> MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments</b></span> got accepted to Robotics and Autonomous Systems (RAS)!!

  - **September 2022:** We proposed <span style="color:#006600"> <b> Instance-wise Grasp Synthesis for Robotic Grasping </b></span>. The paper has been accepted at [(**ICRA 2023**)](https://www.icra2023.org/). 

  - **September 2022:** We proposed <span style="color:#006600"> <b> Agile and Versatile Robot Locomotion via Kernel-based Residual Learning </b></span>. The paper has been accepted at [(**ICRA 2023**)](https://www.icra2023.org/), the video is available online [Here](https://youtu.be/MK_s75UpDAg). 


- **September 2022:** We proposed <span style="color:#006600"> <b> Data-efficient Non-parametric Modelling and Control of an Extensible Soft Manipulator </b></span>.  The paper has been accepted at [(**ICRA 2023**)](https://www.icra2023.org/) and the video is available online [Here](https://youtu.be/_y7LvG-JS4M). 

 - **September 2022:** We proposed <span style="color:#006600"> <b> Throwing Objects into A Moving Basket While Avoiding Obstacles </b></span>. This paper has been accepted at [(**ICRA 2023**)](https://www.icra2023.org/). The paper is available online [Here](https://arxiv.org/pdf/2210.00609.pdf) and the video can be watched at [Here](https://youtu.be/VmIFF__c_84) 

- **July 2022:** We proposed <span style="color:#006600"> <b> Design, Modelling, and Control of an Extensible
Flexible Finger for Robotic Manipulation</b></span>. The paper is under review but the video is available online [Here](https://youtu.be/6k-fM8i5uTo).

 - **July 2022:** We proposed <span style="color:#006600"> <b> MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments</b></span>. The paper is available online [Here](https://arxiv.org/pdf/2103.10997.pdf).

- **July 2022:**  The FCPortugal, a 3D soccer simulation team, a partnership between the universities of Aveiro and Porto, became <span style="color:#FF0000"> <b> world champion in the 3D Simulation League, in the RoboCup 2022</b></span>, held in Thailand. In this league, teams of 11 fully autonomous, simulated humanoid robots play football against each other. It has been my pleasure to contribute to developing walk engines that are agile and robust (final video is available online [Here](https://www.youtube.com/watch?v=foNHQF4uLXQ&ab_channel=BahiaRT) and to read more detail please click [Here](http://wiki.ieeta.pt/wiki/index.php/FC_Portugal_is_World_Champion_at_RoboCup_2022_3D_Simulation_League)).  -->

<!-- - **March 2022:** We proposed a framework for <span style="color:#006600"> <b>Learning Hybrid Locomotion Skills - Learn to Exploit Residual Dynamics and Modulate Model-based Gait Control</b></span>. The paper is available online [Here](https://arxiv.org/pdf/2011.13798). -->

# Research and Publication

<style>
table, tr {border:hidden;}
td, th {border:hidden;}
</style>

<table style="border:hidden;cellspacing=0; cellpadding=0;">

<!-- <style>
th, td {
  border-style:None;}
</style> -->

  <tr>
    <th style="width:45%"></th>
    <th></th>
  </tr>


<tr>
    <td style = ""><img src="./imgs/softManiSim.gif"/></td>
    <td> <b> + SoftManiSim: A Fast Simulation Framework for Multi-Segment Continuum Manipulators Tailored for Robot Learning:</b> <br>
    <p align="justify">
This paper introduces SoftManiSim, a novel simulation framework
for multi-segment continuum manipulators. Existing continuum robot simulators often rely on simplifying assumptions, such as constant curvature bending or ignoring contact forces, to meet real-time simulation and training demands. To
bridge this gap, we propose a robust and rapid mathematical model for continuum robots at the core of SoftManiSim, ensuring precise and adaptable simulations. The framework can integrate with various rigid-body robots, increasing its utility across different robotic platforms. 
</p>
<br> <a href="https://youtu.be/IYqYS4ZQx6k"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://openreview.net/pdf?id=ovjxugn9Q2"> <b>Paper (CoRL2024)</b></a> &emsp; &emsp;
 <a href="https://github.com/MohammadKasaei/SoftManiSim?tab=readme-ov-file"> <b>Code</b></a> 
 </td>
  </tr>

<tr>
    <td style = ""><img src="./imgs/tactip_grasp.gif"/></td>
    <td> <b> + Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data:</b> <br>
    <p align="justify">
This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation. Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback. Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills.
</p>
<br> <a href="https://youtu.be/4Pg29bUBKqs"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2307.04619.pdf"> <b>Paper (IROS)</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/NODE-IL.gif"/></td>
    <td> <b> + Neural ODE-based Imitation Learning (NODE-IL): Data-Efficient Imitation Learning for Long-Horizon Multi-Skill Robot Manipulation:</b> <br>
    <p align="justify">

In robotics, acquiring new skills through Imitation Learning (IL) is crucial for handling diverse complex tasks. However, model-free IL faces data inefficiency and prolonged training time, whereas model-based methods struggle to obtain accurate models. To address these challenges, we developed Neural ODE-based Imitation Learning (NODE-IL), a novel model-based imitation learning framework that employs Neural Ordinary Differential Equations (Neural ODEs) for learning task dynamics and control policies. NODE-IL comprises (1) Dynamic-NODE for learning the continuous differentiable task's transition dynamics model, and (2) Control-NODE for learning a long-horizon control policy in an MPC fashion, which are trained holistically. Extensively evaluated on challenging manipulation tasks, NODE-IL demonstrates significant advantages in data efficiency, requiring less than 70 samples to achieve robust performance. It outperforms Behavioral Cloning from Observation (BCO) and Gaussian Process Imitation Learning (GP-IL) methods, achieving 70\% higher average success rate, and reducing translation errors for high-precision tasks, which demonstrates its robustness and accuracy, as an effective and efficient imitation learning approach for learning complex manipulation tasks.
</p>
<br> <a href=""> <b>Video</b></a> &emsp; &emsp;
 <a href=""> <b>Paper (IROS)</b></a> 
 </td>
  </tr>



<tr>
    <td style = ""><img src="./imgs/jint.png"/></td>
    <td> <b> + Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains: </b> <br>
    <p align="justify">
    Most state-of-the-art approaches treat object recognition and grasping as two separate problems, even though both use visual input. Furthermore, the knowledge of the robot is fixed after the training phase. In such cases, if the robot encounters new object categories, it must be retrained to incorporate new information without catastrophic forgetting. To resolve this problem, we propose a deep learning architecture with an augmented memory capacity to handle open-ended object recognition and grasping simultaneously. In particular, our approach takes multi-views of an object as input and jointly estimates pixel-wise grasp configuration as well as a deep scale- and rotation-invariant representation as output. The obtained representation is then used for open-ended object recognition through a meta-active learning technique. We demonstrate the ability of our approach to grasp never-seen-before objects and to rapidly learn new object categories using very few examples on-site in both simulation and real-world settings. Our approach empowers a robot to acquire knowledge about new object categories using, on average, less than five instances per category and achieve 95% object recognition accuracy and above 91% grasp success rate on (highly) cluttered scenarios in both simulation and real-robot experiments. 
</p>
<br> <a href="https://youtu.be/n9SMpuEkOgk"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://link.springer.com/content/pdf/10.1007/s10846-024-02092-5.pdf"> <b>Paper (JIRS)</b></a>  

 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/push_grasp.gif"/></td>
    <td> <b> + Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios: </b> <br>
    <p align="justify">
    In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile 
actions such as grasping and throwing, within the domain of
robotic manipulation. We introduce an innovative approach
to learning these synergies by leveraging model-free deep
reinforcement learning. The robot’s workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot’s 
operational reach. 
</p>
<br> <a href="https://youtu.be/q1l4BJVDbRw"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2402.16045.pdf"> <b>Paper (ICRA)</b></a>  

 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/lang.png"/></td>
    <td> <b> + Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter :</b> <br>
    <p align="justify">
    Robots operating in human-centric environments require the integration of visual grounding and grasping capabilities to effectively manipulate objects based on user instructions. This work focuses on the task of referring grasp synthesis, which predicts a grasp pose for an object referred through natural language in cluttered scenes. Existing approaches often employ multi-stage pipelines that first segment the referred object and then propose a suitable grasp, and are evaluated in simple datasets or simulators that do not capture the complexity of natural indoor scenes. To address these limitations, we develop a challenging benchmark based on cluttered indoor scenes from OCID dataset, for which we generate referring expressions and connect them with 4-DoF grasp poses. Further, we propose a novel end-to-end model (CROG) that leverages the visual grounding capabilities of CLIP to learn grasp synthesis directly from image-text pairs. Our results show that vanilla integration of CLIP with pretrained models transfers poorly in our challenging benchmark, while CROG achieves significant improvements both in terms of grounding and grasping. 
    Extensive robot experiments in both simulation and hardware demonstrate the effectiveness of our approach in challenging interactive object grasping scenarios that include clutter.
</p>
<br> <a href="https://youtu.be/6tYS-5tkoQg"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://openreview.net/pdf?id=j2AQ-WJ_ze"> <b>Paper (CoRL)</b></a>  &emsp; &emsp;
 <a href="https://github.com/gtziafas/OCID-VLG#ocid-vlg-a-vision-language-grasping-dataset-for-cluttered-indoor-scenes"> <b>Code</b></a>  

 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/corl.gif"/></td>
    <td> <b> + A Data-efficient Neural ODE Framework for Optimal Control of Soft Manipulators:</b> <br>
    <p align="justify">
    This work introduces a novel approach for modeling continuous forward kinematic models of soft continuum robots by employing Augmented Neural ODE (ANODE), a cutting-edge family of deep neural network models. To the best of our knowledge, this is the first application of ANODE in modeling soft continuum robots. This formulation introduces auxiliary dimensions, allowing the system's states to evolve in the augmented space which provides a richer set of dynamics that the model can learn, increasing the flexibility and accuracy of the model. Our methodology achieves exceptional sample efficiency, training the continuous forward kinematic model using only 25 scattered data points.  Additionally, we design and implement a fully parallel Model Predictive Path Integral~(MPPI)-based controller running on a GPU, which efficiently manages a non-convex objective function. Through a set of experiments, we showed that the proposed framework (ANODE+MPPI) significantly outperforms state-of-the-art learning based methods such as FNN and RNN in unseen-before scenarios and marginally outperforms them in seen-before scenarios.
</p>
<br> <a href="https://youtu.be/6tYS-5tkoQg"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://openreview.net/pdf?id=PalhNjBJqv"> <b>Paper (CoRL)</b></a>  &emsp;&emsp;
 <a href="https://github.com/MohammadKasaei/SoftRobotSimulator"> <b>Code</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/bimanual.gif"/></td>
    <td> <b> + Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance Control:</b> <br>
    <p align="justify">
This work presents a novel extension to dual-arm tele-cooperation, leveraging the non-linear stiffness and passivity of Fractal Impedance Control (FIC) to adapt to diverse cooperative scenarios. Unlike traditional impedance controllers, our approach ensures stability without relying on energy tanks, as demonstrated in our prior research. In this work, we further extend the FIC framework to bimanual operations, allowing for stable and smooth switching between different dynamic tasks without gain tuning. We also introduce a telemanipulation architecture that offers higher transparency and dexterity, addressing the challenges of signal latency and low-bandwidth communication.
</p>
<br> <a href="https://www.youtube.com/watch?v=AKGFUzBscRE&t=3s"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2108.04567.pdf"> <b>Paper (ICRA)</b></a> 
 </td>
  </tr>

<tr>
    <td style = ""><img src="./imgs/dual_arm_grasping.gif"/></td>
    <td> <b> + MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments:</b> <br>
    <p align="justify">
In this work, we propose a multi-view deep learning approach to handle robust object grasping in human-centric domains. In particular, our approach takes a point cloud of an arbitrary object as an input, and then, generates orthographic views of the given object. The obtained views are finally used to estimate pixel-wise grasp synthesis for each object. We train the model end-to-end using a synthetic object grasp dataset and test it on both simulation and real-world data without any further fine-tuning.
</p>
<br> <a href="https://youtu.be/r7Ra8BJsAY4"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2103.10997.pdf"> <b>Paper (RAS)</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/TiV-ODE.gif"/></td>
    <td> <b> + TiV-ODE: A Neural ODE-based Approach for Controllable Video Generation From Text-Image Pairs:</b> <br>
    <p align="justify">
The work introduces a novel framework, named TiV-ODE, which enables the generation of highly controllable videos using a static image and text caption. The framework relies on the Neural Ordinary Differential Equations (Neural ODEs) to represent complex underlying dynamical systems through a set of nonlinear ordinary differential equations. The proposed method generates videos that possess both the desired dynamics and content. The experimental results demonstrate the effectiveness of the approach in producing visually consistent and highly controllable videos while modeling dynamical systems. This work represents a significant advancement towards developing advanced models for generating controllable videos that can handle complex and dynamic scenes.
</p>
<!-- <br> <a href="https://youtu.be/4Pg29bUBKqs"> <b>Video</b></a> &emsp; &emsp; -->
 <a href="https://arxiv.org/pdf/2303.05323.pdf"> <b>Paper (ICRA)</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/throwing_objects.gif"/></td>
    <td> <b> + Throwing Objects into A Moving Basket While Avoiding Obstacles:</b> <br>
    <p align="justify">
 In this work, we tackle object throwing problem through a deep reinforcement learning approach that enables robots to precisely throw objects into moving baskets while there are obstacles obstructing the path. To the best of our knowledge, we are the first group that addresses throwing
objects with obstacle avoidance. Such a throwing skill not only increases the physical reachability of a robot arm but also improves the execution time.
</p>
<br> <a href="https://youtu.be/VmIFF__c_84"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2210.00609.pdf"> <b>Paper (ICRA)</b></a> 
 </td>
  </tr>



<tr>
    <td style = ""><img src="./imgs/softRobot.jpg"/></td>
    <td> <b> + Design, Modelling, and Control of an Extensible Flexible Finger for Robotic Manipulation:</b> <br>
    <p align="justify">  This work presents a novel cable-driven soft robot capable of flexing in 3D space with an additional degree of freedom for extension and retraction. In comparison with non-extensible soft robots, the proposed robot provides a larger workspace to reach 3D targeted points. We detail the robot design and prototyping, develop a novel mathematical model for predicting the robot's motion, and employ the model to control the robot that can autonomously follow time-varying trajectories.
</p>
<br> <a href="https://youtu.be/_y7LvG-JS4M"> <b>Video</b></a> &emsp; &emsp;
 <!-- <a href="https://arxiv.org/pdf/2210.00609.pdf"> <b>Paper</b></a>  -->
 </td>
  </tr>

<tr>
    <td style = ""><img src="./imgs/nodeSoftRobot.png"/></td>
    <td> <b> + Data-efficient Non-parametric Modelling and Control of an Extensible Soft Manipulator:</b> <br>
    <p align="justify"> This work proposed a novel data-efficient and non-parametric approach to develop a continuous model using a small dataset of real robot demonstrations (only 25 points). To the best of our knowledge, the proposed approach is the most sample-efficient method for soft continuum robot. Furthermore, we employed this model to develop a  controller to track arbitrary trajectories in the feasible kinematic space. 
</p>
<br> <a href="https://youtu.be/_y7LvG-JS4M"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://ieeexplore.ieee.org/document/10161275"> <b>Paper (ICRA)</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/locoMilo.gif"/></td>
    <td> <b> + Agile and Versatile Robot Locomotion via Kernel-based Residual Learning:</b> <br>
    <p align="justify"> This work developed a kernel-based residual learning framework for quadrupedal robotic locomotion. Initially, a kernel neural network is trained with data collected from an MPC controller. Alongside a frozen kernel network, a residual controller network is trained via reinforcement learning to acquire generalized locomotion skills and resilience against external perturbations. With this proposed framework, a robust quadrupedal locomotion controller is learned with high sample efficiency and controllability, providing omnidirectional locomotion at continuous velocities. Its versatility and robustness are validated on unseen terrains that the expert MPC controller fails to traverse. Furthermore, the learned kernel can produce a range of functional locomotion behaviors and can generalize to unseen gaits.
</p>
<br> <a href="https://youtu.be/MK_s75UpDAg"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2302.07343.pdf"> <b>Paper (ICRA)</b></a> 
 </td>
  </tr>


<tr>
    <td style = ""><img src="./imgs/ssg.png"/></td>
    <td> <b> + Instance-wise Grasp Synthesis for Robotic Grasping:</b> <br>
    <p align="justify"> Generating high-quality instance-wise grasp configurations provides critical information of how to grasp specific objects in a multi-object environment and is of high importance for robot manipulation tasks. This work proposed a novel Single-Stage Grasp (SSG) synthesis network, which performs high-quality instance-wise grasp synthesis in a single stage: instance mask and grasp configurations are generated for each object simultaneously. Our method outperforms state-of-the-art on robotic grasp prediction based on the OCID-Grasp dataset, and performs competitively on the JACQUARD dataset. The benchmarking results showed significant improvements compared to the baseline on the accuracy of generated grasp configurations. 
</p>
<br> 
<a href="https://youtu.be/riBXMgrupUw"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2302.07824.pdf"> <b>Paper (ICRA)</b></a> &emsp; &emsp;
 <a href="https://github.com/HilbertXu/Instance-wise-grasp-synthesis"> <b>Code</b></a> 
 

 </td>
  </tr>


  <tr>
    <td style = ""><img src="./imgs/robocup22.jpg"/></td>
    <td> <b> + FC Portugal: RoboCup 2022 3D Simulation League and Technical Challenge Champions:</b> <br>
    <p align="justify">
     FC Portugal, a team from the universities of Porto and Aveiro, won the main competition of the **2022 RoboCup 3D Simulation League**, with 17 wins, 1 tie and no losses. During the course of the competition, the team scored 84 goals while conceding only 2. FC Portugal also won the 2022 RoboCup 3D Simulation League Technical Challenge, accumulating the maximum amount of points by ending first in its both events: the Free/Scientific Challenge, and the Fat Proxy Challenge. The team presented in this year’s competition was rebuilt from the ground up since the last RoboCup. No previous code was used or adapted, with the exception of the 6D pose estimation algorithm, and the get-up behaviors, which were re-optimized. In comparison with our previous team, the omnidirectional walk is more stable and went from 0.70 m/s to 0.90 m/s, the long kick from 15 m to 19 m, and the new close-control dribble reaches up to 1.41 m/s.
   </p>
<br> <a href="https://www.youtube.com/live/foNHQF4uLXQ?feature=share"> <b>Video</b></a> &emsp; &emsp;
  <a href="https://www.springerprofessional.de/en/fc-portugal-robocup-2022-3d-simulation-league-and-technical-chal/24669132"> <b>Paper</b></a> &emsp; &emsp;<a href="https://archive.robocup.info/Soccer/Simulation/3D/TDPs/RoboCup/2022/FCPortugal_SS3D_RC2022_TDP.pdf"> <b>TDP</b></a> 
 </td>
  </tr>


  <tr>
    <td style = ""><img src="./imgs/humanoidWalking.gif"/></td>
    <td> <b> + Learning Hybrid Locomotion Skills – Learn to Exploit Residual Dynamics and Modulate Model-based Gait Control:</b> <br>
    <p align="justify">
    This work aims to combine machine learning and control approaches for legged robots, and developed a hybrid framework to achieve new capabilities of balancing against external perturbations. The framework embeds a kernel which is a fully parametric closed-loop gait generator based on analytical control. On top of that, a neural network with symmetric partial data augmentation learns to automatically adjust the parameters for the gait kernel and to generate compensatory actions for all joints as the residual dynamics, thus significantly augmenting the stability under unexpected perturbations. </p>
<br> <a href="https://youtu.be/sdcREkRHk-Q"> <b>Video</b></a> &emsp; &emsp;
 <a href="https://arxiv.org/pdf/2011.13798.pdf"> <b>Paper (Frontiers Robotics)</b></a> 
 </td>
 
  </tr>



</table>

# Contact
Dr.Mohammadreza Kasaei\
Bayes Centre - G1.10\
47 Potterrow\
Edinburgh\
EH8 9BT\
Email: m.kasaei[at]ed.ac.uk